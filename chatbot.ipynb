{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "# model, tokenizer = load(\"mlx-community/NeuralBeagle14-7B-4bit-mlx\")\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/NeuralBeagle14-7B-mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a helpfull assistent.\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"tell me a joke about a neural network\"},\n",
    "]\n",
    "chat_prompt = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "print(chat_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate(model, tokenizer, prompt=chat_prompt,\n",
    "                    verbose=False, max_tokens=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the next <|im_end|> in the response and remove everything after it\n",
    "response = response.split(\"<|im_end|>\")[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Functions for chatting with the bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlx_lm import load, generate\n",
    "# model, tokenizer = load(\"mlx-community/NeuralBeagle14-7B-4bit-mlx\")\n",
    "\n",
    "model, tokenizer = load(\"mlx-community/NeuralBeagle14-7B-mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function which takes the response and generates a condensed version of it as context for the next messages\n",
    "context_model, context_tokenizer = load(\n",
    "    \"mlx-community/NeuralBeagle14-7B-4bit-mlx\")\n",
    "\n",
    "\n",
    "def generate_context(response, max_tokens=400):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpfull assistent. Summarizing messages.\",\n",
    "        },\n",
    "        {\"role\": \"message\", \"content\": response},\n",
    "    ]\n",
    "\n",
    "    chat_prompt = context_tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    response = generate(context_model, context_tokenizer, prompt=chat_prompt,\n",
    "                        verbose=False, max_tokens=max_tokens)\n",
    "    \n",
    "    # find the next <|im_end|> in the response and remove everything after it\n",
    "    response = response.split(\"<|im_end|>\")[0]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(prompt, context_vector=None, max_tokens=1000):\n",
    "    # Combine the context vector with the prompt if the context is not empty\n",
    "    if context_vector is not None and len(context_vector) > 0:\n",
    "        combined_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpfull assistent.\"},\n",
    "            {\"role\": \"context\", \"content\":  context_vector}, \n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "    else:\n",
    "        combined_prompt = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpfull assistent.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}]\n",
    "\n",
    "    combined_prompt = tokenizer.apply_chat_template(\n",
    "        combined_prompt, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "    # Generate the response using the model\n",
    "    response = generate(model, tokenizer, prompt=combined_prompt,\n",
    "                        verbose=False, max_tokens=max_tokens)\n",
    "    response = response.split(\"<|im_end|>\")[0]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"tell me a joke about a neural network\", max_tokens=400)\n",
    "print(response)\n",
    "new_context_vector = generate_context(response, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = chat(\"tell me another joke about a neural network\", new_context_vector)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Class for the chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatter_mlx import Chatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# chatbot = Chatter(\"mlx-community/CodeLlama-7b-Python-4bit-MLX\")\n",
    "chatbot = Chatter(\"mlx-community/NeuralBeagle14-7B-4bit-mlx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, context = chatbot.chat(\n",
    "    \"Write me a function which that takes an array of numbers and sorts them in ascending order using the bubble sort algorithm in plain python code.\", max_tokens=800)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response, context = chatbot.chat(\"Now write this function in C++.\", context, max_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chatbot.chat(\"Whats the difference between the algorithms you provided?\", context, max_tokens=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_pg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
