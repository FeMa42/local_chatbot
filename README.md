# local_chatbot

Simple chatbot that can run localy using "mlx_lm" with models from the mLX-Community on HuggingFace (see here: https://huggingface.co/mlx-community)

Install the package using the following command:
'''
pip install mlx-lm
'''

The webpage is generated by the "streamlit" package. To install it, use the following command:
'''
pip install streamlit
'''

To run the chatbot, use the following command:
'''
streamlit run main_mlx.py
'''

This chatbot is based on a tutorial from lightning AI (https://lightning.ai/lightning-ai/studios/run-codellama-70b-instruct?section=featured) using their Lightning Studio. It uses Ollama (http://ollama.ai/) for the LLM in the background.